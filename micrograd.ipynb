{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://cs231n.github.io/assets/nn1/neuron_model.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "\tdef __init__(self, data, _children=(), _op='', label=''):\n",
    "\t\tself.data = data\n",
    "\t\tself.grad = 0.0\n",
    "\t\tself._backward = lambda: None\n",
    "\t\tself._prev = set(_children)\n",
    "\t\tself._op = _op\n",
    "\t\tself.label = label\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"Value({self.data})\"\n",
    "\t\n",
    "\tdef __add__(self, other):\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)\n",
    "\t\tout = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += 1.0 * out.grad\n",
    "\t\t\tother.grad += 1.0 * out.grad\n",
    "\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __radd__(self, other):\n",
    "\t\treturn self + other\n",
    "\n",
    "\tdef __neg__(self):\n",
    "\t\treturn self * -1.0\n",
    "\n",
    "\tdef __sub__(self, other):\n",
    "\t\treturn self + (-other)\n",
    "\t\n",
    "\tdef __mul__(self, other):\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)\n",
    "\t\tout = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += other.data * out.grad\n",
    "\t\t\tother.grad += self.data * out.grad\n",
    "\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __rmul__(self, other):\n",
    "\t\treturn self * other\n",
    "\n",
    "\tdef __pow__(self, other):\n",
    "\t\tassert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "\t\tout = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += (other * self.data ** (other - 1)) * out.grad\n",
    "\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\t\n",
    "\tdef tanh(self):\n",
    "\t\tx = self.data\n",
    "\t\tt = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "\t\tout = Value(t, (self,), 'tanh')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += (1 - t**2) * out.grad\n",
    "\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef exp(self):\n",
    "\t\tx = self.data\n",
    "\t\tout = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += out.data * out.grad\n",
    "\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef topo(self):\n",
    "\t\ttopo = []\n",
    "\t\tvisited = set()\n",
    "\t\tdef build_topo(v):\n",
    "\t\t\tif v not in visited:\n",
    "\t\t\t\tvisited.add(v)\n",
    "\t\t\t\tfor child in v._prev:\n",
    "\t\t\t\t\tbuild_topo(child)\n",
    "\t\t\t\ttopo.append(v)\n",
    "\n",
    "\t\tbuild_topo(self)\n",
    "\t\treturn topo\n",
    "\n",
    "\tdef backward(self):\n",
    "\t\tself.grad = 1.0\n",
    "\n",
    "\t\t# Build the graph in topological order\n",
    "\t\ttopo = []\n",
    "\t\tvisited = set()\n",
    "\t\tdef build_topo(v):\n",
    "\t\t\tif v not in visited:\n",
    "\t\t\t\tvisited.add(v)\n",
    "\t\t\t\tfor child in v._prev:\n",
    "\t\t\t\t\tbuild_topo(child)\n",
    "\t\t\t\ttopo.append(v)\n",
    "\n",
    "\t\tbuild_topo(self)\n",
    "\n",
    "\t\tfor node in reversed(self.topo()):\n",
    "\t\t\tnode._backward()\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "\tdef __init__(self, nin): # number of inputs\n",
    "\t\tself.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "\t\tself.b = Value(random.uniform(-1,1))\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\tact = sum((wi * xi for wi,xi in zip(self.w, x)), self.b)\n",
    "\t\treturn act.tanh()\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "\tdef __init__(self, nin, nout): # number of inputs, number of outputs\n",
    "\t\tself.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\touts = [n(x) for n in self.neurons]\n",
    "\t\treturn outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "\tdef __init__(self, nin, nouts):\n",
    "\t\tsz = [nin] + nouts\n",
    "\t\tself.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "\tdef train(self, xs, ys, rounds=20, learning_rate=0.1):\n",
    "\t\typred = None\n",
    "\t\tfor k in range(rounds):\n",
    "\t\t\t# forward pass\n",
    "\t\t\typred = [n(x) for x in xs]\n",
    "\t\t\tloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "\n",
    "\t\t\t# reset gradients to zero\n",
    "\t\t\tfor p in self.parameters():\n",
    "\t\t\t\tp.grad = 0.0\n",
    "\n",
    "\t\t\t# backward pass\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tfor p in self.parameters():\n",
    "\t\t\t\tp.data += -learning_rate * p.grad\n",
    "\n",
    "\t\t\tprint(k, loss.data)\n",
    "\t\treturn ypred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4, 4, 1])\n",
    "xs = [\n",
    "\t[2.0, 3.0, -1.0],\n",
    "\t[3.0, -1.0, 0.5],\n",
    "]\n",
    "ys = [1, -1.0]\n",
    "n.train(xs, ys, rounds=20, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neurons[0].w[0].grad\n",
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "L.back()\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol():\n",
    "\th = 0.00001\n",
    "\n",
    "\ta = Value(2.0, label='a')\n",
    "\tb = Value(-3.0, label='b')\n",
    "\tc = Value(10.0, label='c')\n",
    "\te = a*b; e.label = 'e'\n",
    "\td = e + c; d.label = 'd'\n",
    "\tf = Value(-2.0, label='f')\n",
    "\tL = d * f; L.label = 'L'\n",
    "\tL1 = L.data\n",
    "\n",
    "\ta = Value(2.0, label='a')\n",
    "\tb = Value(-3.0, label='b')\n",
    "\tc = Value(10.0, label='c')\n",
    "\te = a*b; e.label = 'e'\n",
    "\td = e + c; d.label = 'd'\n",
    "\tf = Value(-2.0, label='f')\n",
    "\tL = d * f; L.label = 'L'\n",
    "\tL2 = L.data + h\n",
    "\n",
    "\tprint((L1 - L2) / h)\n",
    "\n",
    "lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,Ã—2\n",
    "x1 = Value (2.0, label='x1')\n",
    "x2 = Value (0.0, label= 'x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value (1.0, label= 'w2')\n",
    "# bias of the neuron\n",
    "b = Value (6.88137358790, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1xw1' \n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'xl*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "o.back()\n",
    "draw_dot(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
